{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d616603-ba37-4225-b96d-7273e5a9832b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import pickle\n",
    "import numpy as np\n",
    "from datetime import datetime as dt\n",
    "import time\n",
    "import pandas as pd\n",
    "from beautifultable import BeautifulTable\n",
    "from sklearn.ensemble import *\n",
    "from sklearn.model_selection import *\n",
    "from sklearn.metrics import *\n",
    "from sklearn.preprocessing import *\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0a365aa-2cc7-40dc-a0fc-f2049c34d508",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\" Functions for model set up \"\"\"\n",
    "\n",
    "def get_features(fmat, label_cols, feature_file=None, n_feats2sel=None):\n",
    "    all_cols = fmat.columns.values.tolist()\n",
    "    if not feature_file:\n",
    "        data_cols = [c for c in all_cols if c not in label_cols]\n",
    "    else:\n",
    "        fsel = pd.read_csv(feature_file)\n",
    "        select_feats = fsel['feature'].head(int(n_feats2sel)).tolist()\n",
    "        data_cols = [c for c in all_cols if c not in label_cols and c in select_feats]\n",
    "    return(data_cols)\n",
    "    \n",
    "def fmt_data(fmat, subset, label_cols, data_cols, keep_groups=True):\n",
    "    # get desired subset\n",
    "    fmat_fmt = fmat[fmat['label'].isin(subset)]\n",
    "    fmat_fmt.reset_index(inplace=True, drop=True)\n",
    "    # format data for sklearn input\n",
    "    X = fmat_fmt[data_cols].to_numpy()\n",
    "    y = fmat_fmt[label_cols[1]].to_numpy()\n",
    "    ids = fmat_fmt[label_cols[0]]\n",
    "    # keep/drop group col option\n",
    "    if keep_groups:\n",
    "        groups = fmat_fmt[label_cols[2]].to_numpy()\n",
    "        return(X, y, ids, groups)\n",
    "    else:\n",
    "        return(X, y, ids)\n",
    "    \n",
    "def fmt_outdir(outdir):\n",
    "    if outdir.endswith('/'):\n",
    "        return(outdir)\n",
    "    else:\n",
    "        return(outdir+'/')\n",
    "\n",
    "def def_grp_split(method='GroupShuffleSplit', num_splits=5, train_size=0.7, seed=None):\n",
    "    if method == 'GroupShuffleSplit':\n",
    "        gs = GroupShuffleSplit(n_splits = num_splits, train_size=train_size, random_state=seed)\n",
    "    elif method == 'GroupKFold':\n",
    "        gs = GroupKFold(n_splits = num_splits)\n",
    "    elif method == 'StratifiedGroupKFold':\n",
    "        gs = StratifiedGroupKFold(n_splits = num_splits)\n",
    "    else:\n",
    "        print('Invalid group split strategy specified; please choose one of: \"GroupShuffleSplit\", \"GroupKFold\", or \"StratifiedGroupKFold\"')\n",
    "        print('Review documentation for more information on each method: https://scikit-learn.org/stable/modules/classes.html#module-sklearn.model_selection')\n",
    "    return(gs)\n",
    "\n",
    "def load_model(model_file, seed=None):\n",
    "    with open(model_file, 'rb') as f:\n",
    "        model = pickle.load(f)\n",
    "    try:\n",
    "        n_steps = len(model.named_steps)\n",
    "        idx = n_steps-1\n",
    "        model_name = type(model[idx]).__name__\n",
    "        if seed:\n",
    "            setattr(model[idx], 'random_state', seed)\n",
    "    except:\n",
    "        model_name = type(model).__name__\n",
    "        if seed:\n",
    "            try:\n",
    "                setattr(model, 'random_state', seed)\n",
    "            except:\n",
    "                print(f'WARNING: failed to set random state for {model} to {seed}!')\n",
    "    return(model, model_name)\n",
    "\n",
    "def split_data(X, y, train_idx, test_idx):\n",
    "    # get splits\n",
    "    X_train = X[train_idx]\n",
    "    y_train = y[train_idx]\n",
    "    X_test = X[test_idx]\n",
    "    y_test = y[test_idx]\n",
    "    # check split & label balance\n",
    "    label, counts = np.unique(y_train, return_counts=True)\n",
    "    label_counts_train = dict(zip(label, counts))\n",
    "    label, counts = np.unique(y_test, return_counts=True)\n",
    "    label_counts_test = dict(zip(label, counts))\n",
    "    print(f' ► # train PPIs = {len(X_train)}')\n",
    "    print(f' ► train +/- label counts: {label_counts_train}')\n",
    "    print(f' ► # test PPIs = {len(X_test)}')\n",
    "    print(f' ► test +/- label counts: {label_counts_test}')\n",
    "    return(X_train, y_train, X_test, y_test)\n",
    "\n",
    "def write_results(all_res, test_scores, model_name, outdir, fdr_cutoff):\n",
    "    # format path, file names\n",
    "    outdir_fmt = fmt_outdir(outdir)\n",
    "    fdr_fmt = int(fdr_cutoff*100)\n",
    "    bases = ['scored_interactions_all', f'scored_interactions_fdr{fdr_fmt}', 'precision_recall' ]\n",
    "    all_out = outdir_fmt+f'{bases[0]}_{model_name}.csv'\n",
    "    high_conf_out = outdir_fmt+f'{bases[1]}_{model_name}.csv'\n",
    "    pr_out = outdir_fmt+f'{bases[2]}_{model_name}.csv'\n",
    "    # format all scored PPIs\n",
    "    all_res.sort_values('ppi_score', inplace=True, ascending=False)\n",
    "    all_res.reset_index(inplace=True, drop=True)\n",
    "    print(f' ► Writing all scored PPIs to {all_out} ...')\n",
    "    all_res.to_csv(all_out, index=False)\n",
    "    # get PR curve & threshold PPIs at given FDR\n",
    "    pr_curve, high_conf_ppis = threshold_ppis(test_scores, all_res, fdr_cutoff)\n",
    "    print(f' ► Writing precision-recall results to {pr_out} ...')\n",
    "    pr_curve.to_csv(pr_out)\n",
    "    print(f' ► Writing results to {high_conf_out} ...')\n",
    "    high_conf_ppis.to_csv(high_conf_out, index=False)\n",
    "\n",
    "\"\"\" Functions for model fitting & evaluation \"\"\"\n",
    "\n",
    "def calc_pr(df):\n",
    "    # compute precision/recall\n",
    "    print(f\" ► Computing precision/recall ...\")\n",
    "    tp_count = 0\n",
    "    fp_count = 0\n",
    "    p_list = []\n",
    "    r_list = []\n",
    "    f_list = []\n",
    "    all_pos = len(df[df['label'] == 1])\n",
    "    for i in range(len(df)):\n",
    "        if df['label'][i] == 1:\n",
    "            tp_count += 1\n",
    "        else:\n",
    "            fp_count += 1\n",
    "        tps = tp_count\n",
    "        fps = fp_count\n",
    "        fns = all_pos - tps\n",
    "        precision = tps/(tps+fps)\n",
    "        recall = tps/(tps+fns)\n",
    "        fdr = 1 - precision\n",
    "        p_list.append(float(precision))\n",
    "        r_list.append(float(recall))\n",
    "        f_list.append(float(fdr))\n",
    "    df['precision'] = p_list\n",
    "    df['recall'] = r_list\n",
    "    df['fdr'] = f_list\n",
    "    return(df)\n",
    "\n",
    "def threshold_ppis(test_scores, all_res, fdr_cutoff):\n",
    "    # compute precision/recall\n",
    "    test_scores_pr = calc_pr(test_scores)\n",
    "    thres_df = test_scores_pr[test_scores_pr['fdr'] <= fdr_cutoff]\n",
    "    prob_cutoff = min(thres_df['ppi_score'])\n",
    "    print(f' ► PPI score cutoff for {int(fdr_cutoff*100)}% FDR: {prob_cutoff}')\n",
    "    # theshold results\n",
    "    thres_df = all_res[all_res['ppi_score'] >= prob_cutoff]\n",
    "    ids = thres_df['ID'].str.split(' ', expand=True)\n",
    "    print(f' ► # total PPIs evaluated: {len(all_res)}')\n",
    "    print(f' ► # PPIs above threshold: {len(thres_df)}')\n",
    "    try: # only works if there are no self-self PPIs\n",
    "        uniq_prots = np.unique(ids[[0, 1]].values)\n",
    "        print(f' ► # unique proteins above threshold: {len(uniq_prots)}')\n",
    "    except:\n",
    "        print('WARNING: Problem with IDs detected.')\n",
    "        print(ids[0:51])\n",
    "    df_out = thres_df[['ID','ppi_score']]\n",
    "    return(test_scores_pr, df_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "044cfdaa-551a-44d9-ba28-cfd0ca6b86fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "featmat = '../ppi_ml/data/featmats/featmat_labeled.pkl'\n",
    "num_splits = 1\n",
    "train_size = 0.7\n",
    "seed = 17\n",
    "remove_per_step = 3\n",
    "threads = 12\n",
    "model = '../ppi_ml/results/tpot/gkfold/tpot_model_3.pkl'\n",
    "group_split_method = 'GroupShuffleSplit'\n",
    "fdr_cutoff = 0.1\n",
    "feature_selection = None\n",
    "num_feats = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "158b846a-5e6b-461c-8379-612a110eb8b7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-03-23 11:59:09.563588] Loading feature matrix ...\n",
      "[2023-03-23 11:59:42.800752] Loading complex group split method parameters ...\n",
      "[2023-03-23 11:59:42.801077] Loading model parameters ...\n",
      "\n",
      "[2023-03-23 11:59:42.820455] Pipeline settings detected:\n",
      "+--------------------------------+-------------------+\n",
      "|           PARAMETER            |      SETTING      |\n",
      "+--------------------------------+-------------------+\n",
      "|  Protein complex split method  | GroupShuffleSplit |\n",
      "+--------------------------------+-------------------+\n",
      "|    # train/test (CV) splits    |         1         |\n",
      "+--------------------------------+-------------------+\n",
      "|    Machine learning method     |     LinearSVC     |\n",
      "+--------------------------------+-------------------+\n",
      "| % data for training (approx*): |        70%        |\n",
      "+--------------------------------+-------------------+\n",
      "| % data for testing (approx*):  |        30%        |\n",
      "+--------------------------------+-------------------+\n",
      "|        % FDR threshold         |        10%        |\n",
      "+--------------------------------+-------------------+\n",
      "|     # features to be used      |        663        |\n",
      "+--------------------------------+-------------------+\n",
      "*Note: Group split method will attempt to get as close to these settings as possible, but results may vary depending on the seed, group sizes, and number of cross-validation splits specified.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/stor/home/rmcox/miniconda3/envs/dev/lib/python3.10/site-packages/sklearn/base.py:299: UserWarning: Trying to unpickle estimator SelectPercentile from version 1.2.2 when using version 1.2.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/stor/home/rmcox/miniconda3/envs/dev/lib/python3.10/site-packages/sklearn/base.py:299: UserWarning: Trying to unpickle estimator LinearSVC from version 1.2.2 when using version 1.2.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/stor/home/rmcox/miniconda3/envs/dev/lib/python3.10/site-packages/sklearn/base.py:299: UserWarning: Trying to unpickle estimator Pipeline from version 1.2.2 when using version 1.2.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "## read in data & define model params\n",
    "print(f'[{dt.now()}] Loading feature matrix ...')\n",
    "with open(featmat, 'rb') as handle:\n",
    "    fmat = pickle.load(handle)\n",
    "\n",
    "label_cols = ['ID', 'label', 'super_group']\n",
    "data_cols = get_features(fmat, label_cols, feature_selection, num_feats)\n",
    "X, y, ids, groups = fmt_data(fmat, [1,-1], label_cols, data_cols, keep_groups=True)\n",
    "X_pred, y_pred, ids_pred = fmt_data(fmat, [0], label_cols, data_cols, keep_groups=False)\n",
    "\n",
    "print(f'[{dt.now()}] Loading complex group split method parameters ...')\n",
    "gs = def_grp_split(group_split_method, num_splits, train_size, seed)\n",
    "\n",
    "print(f'[{dt.now()}] Loading model parameters ...')\n",
    "model, model_name = load_model(model, seed)\n",
    "\n",
    "table = BeautifulTable()\n",
    "table.columns.header = [\"PARAMETER\", \"SETTING\"]\n",
    "table.rows.append([\"Protein complex split method\", group_split_method])\n",
    "table.rows.append([\"# train/test (CV) splits\", num_splits])\n",
    "table.rows.append([\"Machine learning method\", model_name])\n",
    "table.rows.append([\"% data for training (approx*):\", f'{int(train_size*100)}%'])\n",
    "table.rows.append([\"% data for testing (approx*):\", f'{int((1-train_size)*100)}%'])\n",
    "table.rows.append([\"% FDR threshold\", f'{int(fdr_cutoff*100)}%'])\n",
    "table.rows.append([\"# features to be used\", len(data_cols)])\n",
    "\n",
    "print()\n",
    "print(f'[{dt.now()}] Pipeline settings detected:')\n",
    "print(table)\n",
    "print('*Note: Group split method will attempt to get as close to these settings as possible, but results may vary depending on the seed, group sizes, and number of cross-validation splits specified.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a0287e1-1bb8-4712-a6cf-b84f3740ce80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores(model, array, labels, ids):\n",
    "    # get scores\n",
    "    scores = model.predict_proba(array)\n",
    "    probabilities = np.split(scores, 2, axis=1)\n",
    "    neg_prob = probabilities[0]\n",
    "    pos_prob = probabilities[1]\n",
    "    # format into df\n",
    "    df = pd.DataFrame()\n",
    "    df['ID'] = ids\n",
    "    df['label'] = labels\n",
    "    df['ppi_score'] = pos_prob\n",
    "    df.sort_values('ppi_score', inplace=True, ascending=False)\n",
    "    df.reset_index(inplace=True, drop=True)\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f793e75-96c6-4af5-b926-5b5b772d44a3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[2023-03-23 11:59:42.844905] Getting test/train splits (1/1) ...\n",
      " ► # train PPIs = 24405\n",
      " ► train +/- label counts: {-1: 18276, 1: 6129}\n",
      " ► # test PPIs = 2111\n",
      " ► test +/- label counts: {-1: 1611, 1: 500}\n",
      "[2023-03-23 11:59:42.928535] Fitting LinearSVC ...\n",
      "[2023-03-23 11:59:45.973481] Getting probability scores ...\n"
     ]
    }
   ],
   "source": [
    "## fit model, compute precision/recall, and output results\n",
    "for i, (train_idx, test_idx) in enumerate(gs.split(X, y, groups)):\n",
    "\n",
    "    # get train/test splits\n",
    "    print()\n",
    "    print(f\"[{dt.now()}] Getting test/train splits ({i+1}/{num_splits}) ...\")\n",
    "    X_train, y_train, X_test, y_test = split_data(X, y, train_idx, test_idx)\n",
    "\n",
    "    # fit model\n",
    "    print(f\"[{dt.now()}] Fitting {model_name} ...\")\n",
    "    \n",
    "    # extract results\n",
    "    test_ids = ids[test_idx]\n",
    "    train_ids = ids[train_idx]\n",
    "    pred_ids = ids_pred\n",
    "\n",
    "    if len(test_ids) > len(train_ids) and num_splits == 1:\n",
    "        print(\"ERROR: Imbalanced train/test split. Try a different seed using the --seed argument.\")\n",
    "\n",
    "    print(f\"[{dt.now()}] Getting probability scores ...\")\n",
    "    test_scores = get_scores(model, X_test, y_test, test_ids)\n",
    "    train_scores = get_scores(model, X_train, y_train, train_ids)\n",
    "    pred_scores = get_scores(model, X_pred, y_pred, pred_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92e121a3-6562-4e54-a409-de3ffe41c42b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 9.9110e-01,  4.6291e+00, -1.7700e-02, ...,  5.4000e-01,\n",
       "         4.3660e-01,  7.4130e-01],\n",
       "       [ 9.9720e-01,  6.0172e+00, -5.8400e-02, ...,  1.6800e-02,\n",
       "        -1.3500e-02,  6.9760e-01],\n",
       "       [ 9.3580e-01,  8.2533e+00, -2.4000e-02, ...,  0.0000e+00,\n",
       "         0.0000e+00,  0.0000e+00],\n",
       "       ...,\n",
       "       [ 0.0000e+00,  0.0000e+00,  0.0000e+00, ...,  2.2500e-02,\n",
       "        -6.4000e-03,  7.5360e-01],\n",
       "       [ 0.0000e+00,  0.0000e+00,  0.0000e+00, ...,  1.6800e-02,\n",
       "         7.4000e-03,  6.7870e-01],\n",
       "       [ 0.0000e+00,  0.0000e+00,  0.0000e+00, ...,  0.0000e+00,\n",
       "         0.0000e+00,  0.0000e+00]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
